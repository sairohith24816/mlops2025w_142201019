{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c4bc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=Path(\".env\"), override=False)\n",
    "\n",
    "PROJECT = os.environ.get(\"WANDB_PROJECT\", \"tinyimagenet-resnet\")\n",
    "ENTITY = os.environ.get(\"WANDB_ENTITY\", \"sairohith\")\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./tiny-imagenet-200\")\n",
    "EPOCHS = int(os.environ.get(\"EPOCHS\", 3))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 128))\n",
    "LR = float(os.environ.get(\"LR\", 1e-3))\n",
    "IMG_SIZE = int(os.environ.get(\"IMG_SIZE\", 64))\n",
    "DATA_FRACTION = float(os.environ.get(\"DATA_FRACTION\", 0.01))\n",
    "DRIFT_THRESHOLD = float(os.environ.get(\"DRIFT_THRESHOLD\", 0.1))\n",
    "SAVE_PATH = os.environ.get(\"SAVE_PATH\", \"best_resnet_wandb.pth\")\n",
    "RUN_NAME = os.environ.get(\"RUN_NAME\", \"resnet18_run1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89f02e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1621be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# ---------------- transforms ----------------\n",
    "def get_transforms(img_size=64, train=True):\n",
    "    if train:\n",
    "        return T.Compose([\n",
    "            T.RandomResizedCrop(img_size),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "        ])\n",
    "    return T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1d7a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- dataloaders ----------------\n",
    "def load_tiny_imagenet_dataloaders(data_dir, batch_size=128, img_size=64, num_workers=4, fraction=1.0, seed=42):\n",
    "    data_dir = Path(data_dir)\n",
    "    if not 0 < fraction <= 1:\n",
    "        raise ValueError(\"fraction must be in (0, 1].\")\n",
    "    random.seed(seed)\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # train uses ImageFolder structure: train/<wnid>/images/*.JPEG\n",
    "    train_dir = data_dir / \"train\"\n",
    "    train_tf = get_transforms(img_size, train=True)\n",
    "    train_ds = datasets.ImageFolder(root=str(train_dir), transform=train_tf)\n",
    "\n",
    "    if fraction < 1.0:\n",
    "        per_class_indices = {}\n",
    "        for idx, (_, label) in enumerate(train_ds.samples):\n",
    "            per_class_indices.setdefault(label, []).append(idx)\n",
    "        subset_indices = []\n",
    "        for indices in per_class_indices.values():\n",
    "            take = max(1, math.ceil(len(indices) * fraction))\n",
    "            subset_indices.extend(rng.sample(indices, take))\n",
    "        subset_indices.sort()\n",
    "        train_data = Subset(train_ds, subset_indices)\n",
    "    else:\n",
    "        train_data = train_ds\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # validation: read val_annotations and precompute tensors (simple)\n",
    "    val_img_dir = data_dir / \"val\" / \"images\"\n",
    "    ann_file = data_dir / \"val\" / \"val_annotations.txt\"\n",
    "    val_tf = get_transforms(img_size, train=False)\n",
    "\n",
    "    # Use the correct mapping provided by ImageFolder\n",
    "    wnid_to_idx = train_ds.class_to_idx\n",
    "    tensors = []\n",
    "    labels = []\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2: \n",
    "                continue\n",
    "            img_name, wnid = parts[0], parts[1]\n",
    "            p = val_img_dir / img_name\n",
    "            if not p.exists(): \n",
    "                continue\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            tensors.append(val_tf(img))\n",
    "            labels.append(wnid_to_idx[wnid])\n",
    "    if len(tensors) == 0:\n",
    "        raise RuntimeError(\"No validation images found — check data_dir layout.\")\n",
    "    x = torch.stack(tensors)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    val_dataset = TensorDataset(x, y)\n",
    "    if fraction < 1.0:\n",
    "        per_class_val = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            per_class_val.setdefault(label, []).append(idx)\n",
    "        val_indices = []\n",
    "        for indices in per_class_val.values():\n",
    "            take = max(1, math.ceil(len(indices) * fraction))\n",
    "            val_indices.extend(rng.sample(indices, take))\n",
    "        val_indices.sort()\n",
    "        val_dataset = Subset(val_dataset, val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, len(train_ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62adabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------- model ----------------\n",
    "def build_pretrained_resnet(num_classes, device=None):\n",
    "    try:\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    except Exception:\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    in_f = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_f, num_classes)\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return model.to(device), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2986d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------- training / eval ----------------\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in tqdm(loader, leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d56c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------- full training with W&B ----------------\n",
    "def train_resnet_with_wandb(data_dir,\n",
    "                           project=\"tinyimagenet-resnet\",\n",
    "                           entity=None,\n",
    "                           run_name=None,\n",
    "                           epochs=10,\n",
    "                           batch_size=128,\n",
    "                           img_size=64,\n",
    "                           lr=1e-3,\n",
    "                           fraction=1.0,\n",
    "                           save_path=\"best_resnet_wandb.pth\"):\n",
    "    # init wandb\n",
    "    config = dict(epochs=epochs, batch_size=batch_size, img_size=img_size, lr=lr, data_dir=str(data_dir), data_fraction=fraction)\n",
    "    run = wandb.init(project=project, entity=entity, name=run_name, config=config)\n",
    "    cfg = run.config\n",
    "\n",
    "    # data + model\n",
    "    train_loader, val_loader, num_classes = load_tiny_imagenet_dataloaders(cfg.data_dir, batch_size=cfg.batch_size, img_size=cfg.img_size, fraction=cfg.data_fraction)\n",
    "    model, device = build_pretrained_resnet(num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    base_dataset = train_loader.dataset.dataset if isinstance(train_loader.dataset, Subset) else train_loader.dataset\n",
    "    class_names = getattr(base_dataset, \"classes\", [])\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        run.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train/loss\": train_loss, \"train/acc\": train_acc,\n",
    "            \"val/loss\": val_loss, \"val/acc\": val_acc,\n",
    "            \"lr\": scheduler.get_last_lr()[0] if hasattr(scheduler, \"get_last_lr\") else optimizer.param_groups[0][\"lr\"],\n",
    "            \"data_fraction\": cfg.data_fraction\n",
    "        })\n",
    "        print(f\"Epoch {epoch}/{cfg.epochs}  train_acc={train_acc:.4f}  val_acc={val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        scheduler.step()\n",
    "\n",
    "    checkpoint_path = None\n",
    "    if save_path:\n",
    "        cpu_state = {k: v.cpu() for k, v in best_state.items()}\n",
    "        checkpoint = {\n",
    "            \"epoch\": best_epoch,\n",
    "            \"model_state\": cpu_state,\n",
    "            \"val_acc\": best_val_acc,\n",
    "            \"class_names\": class_names\n",
    "        }\n",
    "        torch.save(checkpoint, save_path)\n",
    "        checkpoint_path = save_path\n",
    "        art = wandb.Artifact(\"resnet18-tinyimagenet\", type=\"model\")\n",
    "        art.add_file(save_path)\n",
    "        run.log_artifact(art)\n",
    "        print(f\"Saved best checkpoint from epoch {best_epoch} -> {save_path}\")\n",
    "\n",
    "    run.summary[\"best_val_acc\"] = best_val_acc\n",
    "    run.finish()\n",
    "    return best_val_acc, checkpoint_path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f65320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef880343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024022-6msyrns7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/6msyrns7' target=\"_blank\">resnet18_run1</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/6msyrns7' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/6msyrns7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]C:\\Users\\sairo\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\sairo\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3  train_acc=0.0126  val_acc=0.0290\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3  train_acc=0.0652  val_acc=0.0780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3  train_acc=0.1359  val_acc=0.1410\n",
      "\n",
      "Saved best checkpoint from epoch 3 -> best_resnet_wandb.pth\n",
      "Saved best checkpoint from epoch 3 -> best_resnet_wandb.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data_fraction</td><td>▁▁▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▁▁</td></tr><tr><td>train/acc</td><td>▁▄█</td></tr><tr><td>train/loss</td><td>█▄▁</td></tr><tr><td>val/acc</td><td>▁▄█</td></tr><tr><td>val/loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.141</td></tr><tr><td>data_fraction</td><td>0.1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>train/acc</td><td>0.1359</td></tr><tr><td>train/loss</td><td>4.40628</td></tr><tr><td>val/acc</td><td>0.141</td></tr><tr><td>val/loss</td><td>4.14922</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet18_run1</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/6msyrns7' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/6msyrns7</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024022-6msyrns7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.141\n",
      "Checkpoint saved to: best_resnet_wandb.pth\n",
      " 0.141\n",
      "Checkpoint saved to: best_resnet_wandb.pth\n"
     ]
    }
   ],
   "source": [
    "best_acc, checkpoint_path = train_resnet_with_wandb(data_dir=DATA_DIR,\n",
    "                                                      project=PROJECT,\n",
    "                                                      run_name=RUN_NAME,\n",
    "                                                      epochs=EPOCHS,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      img_size=IMG_SIZE,\n",
    "                                                      lr=LR,\n",
    "                                                      fraction=DATA_FRACTION,\n",
    "                                                      save_path=SAVE_PATH)\n",
    "print(\"Best val accuracy:\", best_acc)\n",
    "print(\"Checkpoint saved to:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98125b",
   "metadata": {},
   "source": [
    "## Deploy to Hugging Face Space\n",
    "Follow the cells below to package the trained checkpoint as a Gradio app and push it to a Hugging Face Space.\n",
    "\n",
    "**High-level flow:**\n",
    "1. Make sure the training cell above finished and saved `best_resnet_wandb.pth` (or another checkpoint).\n",
    "2. Configure the required secrets (Hugging Face access token, W&B entity/project/artifact).\n",
    "3. Generate the Gradio app (`app.py`) that knows how to download the artifact and run inference.\n",
    "4. Create the `requirements.txt` file for the Space.\n",
    "5. Push both files to a new (or existing) Hugging Face Space using `huggingface_hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84bf4a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HF_TOKEN from environment (.env): ***\n",
      "Using HF_USER from environment (.env): SaiRohith24816\n",
      "Using SPACE_NAME from environment (.env): tinyimagenet-demo\n",
      "Using WANDB_ENTITY from environment (.env): ir2023\n",
      "Using WANDB_PROJECT from environment (.env): tinyimagenet-resnet\n",
      "Using WANDB_ARTIFACT from environment (.env): ir2023/tinyimagenet-resnet/resnet18-tinyimagenet:latest\n",
      "Environment variables ready for Hugging Face upload.\n",
      "Using HF_USER from environment (.env): SaiRohith24816\n",
      "Using SPACE_NAME from environment (.env): tinyimagenet-demo\n",
      "Using WANDB_ENTITY from environment (.env): ir2023\n",
      "Using WANDB_PROJECT from environment (.env): tinyimagenet-resnet\n",
      "Using WANDB_ARTIFACT from environment (.env): ir2023/tinyimagenet-resnet/resnet18-tinyimagenet:latest\n",
      "Environment variables ready for Hugging Face upload.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=Path(\".env\"), override=False)\n",
    "\n",
    "def _ensure_env(var_name, prompt_text, is_secret=False, default=\"\"):\n",
    "    current = os.environ.get(var_name, default)\n",
    "    if current:\n",
    "        masked = \"***\" if is_secret else current\n",
    "        print(f\"Using {var_name} from environment (.env): {masked}\")\n",
    "        os.environ[var_name] = current\n",
    "        return current\n",
    "    if is_secret:\n",
    "        value = getpass(prompt_text).strip()\n",
    "    else:\n",
    "        value = input(prompt_text).strip()\n",
    "    if value:\n",
    "        os.environ[var_name] = value\n",
    "    return value\n",
    "\n",
    "hf_token = _ensure_env(\"HF_TOKEN\", \"HF token (input hidden): \", is_secret=True)\n",
    "hf_user = _ensure_env(\"HF_USER\", \"Hugging Face username: \")\n",
    "space_name = _ensure_env(\"SPACE_NAME\", \"Desired Space name (e.g., tinyimagenet-demo): \")\n",
    "\n",
    "_ensure_env(\"WANDB_ENTITY\", \"W&B entity (username/team used during training): \", default=ENTITY)\n",
    "_ensure_env(\"WANDB_PROJECT\", \"W&B project name: \", default=PROJECT)\n",
    "\n",
    "default_artifact = \"\"\n",
    "if os.environ.get(\"WANDB_ENTITY\") and os.environ.get(\"WANDB_PROJECT\"):\n",
    "    default_artifact = f\"{os.environ['WANDB_ENTITY']}/{os.environ['WANDB_PROJECT']}/resnet18-tinyimagenet:latest\"\n",
    "_ensure_env(\"WANDB_ARTIFACT\", \"W&B artifact path [entity/project/name:alias]: \", default=default_artifact)\n",
    "\n",
    "print(\"Environment variables ready for Hugging Face upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02f03f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gradio huggingface_hub git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48f955",
   "metadata": {},
   "source": [
    "### Generate Gradio app\n",
    "The cell below creates an `hf_space` folder (safe to re-run) and writes `app.py`. The app will:\n",
    "- Download the checkpoint from the configured W&B artifact if `MODEL_PATH` is missing.\n",
    "- Rebuild the Tiny-ImageNet ResNet-18, load weights, and expose a Gradio interface.\n",
    "- Display top-1 prediction, confidence, and inference latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "915f0ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote hf_space\\app.py\n",
      "\n",
      "Copied checkpoint to hf_space\\best_resnet_wandb.pth\n",
      "Copied checkpoint to hf_space\\best_resnet_wandb.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import textwrap\n",
    "import shutil\n",
    "\n",
    "hf_dir = Path(\"hf_space\")\n",
    "hf_dir.mkdir(exist_ok=True)\n",
    "app_path = hf_dir / \"app.py\"\n",
    "app_code = textwrap.dedent(\"\"\"\\\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import gradio as gr\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(dotenv_path=Path(\".env\"), override=False)\n",
    "except Exception as err:\n",
    "    print(f\"Could not load .env file: {err}\")\n",
    "\n",
    "MODEL_PATH = Path(os.environ.get(\"MODEL_PATH\", \"best_resnet_wandb.pth\"))\n",
    "ARTIFACT_PATH = os.environ.get(\"WANDB_ARTIFACT\")\n",
    "DOWNLOAD_ROOT = Path(os.environ.get(\"MODEL_DOWNLOAD_DIR\", \"./downloaded_artifacts\"))\n",
    "\n",
    "def maybe_download_artifact():\n",
    "    if MODEL_PATH.exists():\n",
    "        return\n",
    "    if not ARTIFACT_PATH:\n",
    "        print(\"MODEL_PATH missing and WANDB_ARTIFACT not set -- cannot download weights.\")\n",
    "        return\n",
    "    try:\n",
    "        import wandb\n",
    "        api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "        if api_key:\n",
    "            wandb.login(key=api_key)\n",
    "        else:\n",
    "            wandb.login()\n",
    "        api = wandb.Api()\n",
    "        artifact = api.artifact(ARTIFACT_PATH)\n",
    "        DOWNLOAD_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "        artifact_dir = Path(artifact.download(root=str(DOWNLOAD_ROOT), recursive=False))\n",
    "        candidate = artifact_dir / MODEL_PATH.name\n",
    "        if candidate.exists():\n",
    "            candidate.replace(MODEL_PATH)\n",
    "        else:\n",
    "            matches = sorted(artifact_dir.glob(\"*.pth\"))\n",
    "            if matches:\n",
    "                matches[0].replace(MODEL_PATH)\n",
    "        print(f\"Downloaded artifact files to {artifact_dir}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Failed to download artifact: {err}\")\n",
    "\n",
    "def load_class_names(ckpt) -> List[str]:\n",
    "    names = ckpt.get(\"class_names\")\n",
    "    if isinstance(names, list) and len(names) > 0:\n",
    "        return names\n",
    "    wnids_file = Path(\"./tiny-imagenet-200/wnids.txt\")\n",
    "    if wnids_file.exists():\n",
    "        return [line.strip() for line in wnids_file.read_text().splitlines() if line.strip()]\n",
    "    return [f\"class_{idx}\" for idx in range(200)]\n",
    "\n",
    "def build_model(num_classes: int) -> nn.Module:\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def format_top_k(probs: torch.Tensor, class_names: List[str], k: int = 5) -> Dict[str, float]:\n",
    "    scores = probs.squeeze(0)\n",
    "    k = min(k, scores.numel())\n",
    "    top_scores, top_indices = torch.topk(scores, k)\n",
    "    formatted: Dict[str, float] = {}\n",
    "    for score, idx in zip(top_scores, top_indices):\n",
    "        label = class_names[idx] if 0 <= idx < len(class_names) else f\"class_{int(idx)}\"\n",
    "        formatted[label] = float(score)\n",
    "    return formatted\n",
    "\n",
    "def main():\n",
    "    maybe_download_artifact()\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state\", checkpoint)\n",
    "    class_names = load_class_names(checkpoint)\n",
    "    model = build_model(len(class_names))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize((64, 64)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    def predict(image: Image.Image):\n",
    "        start = time.time()\n",
    "        x = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "        elapsed = (time.time() - start) * 1000.0\n",
    "        topk_dict = format_top_k(probs, class_names, k=5)\n",
    "        top_label = next(iter(topk_dict)) if topk_dict else \"unknown\"\n",
    "        print(f\"Predicted {top_label} in {elapsed:.2f} ms\")\n",
    "        return topk_dict\n",
    "\n",
    "    title = \"Tiny-ImageNet ResNet18\"\n",
    "    description = (\"Upload an image to see the top-5 Tiny-ImageNet predictions from the fine-tuned ResNet18 model.\")\n",
    "    gr.Interface(\n",
    "        fn=predict,\n",
    "        inputs=gr.Image(type=\"pil\"),\n",
    "        outputs=gr.Label(num_top_classes=5),\n",
    "        title=title,\n",
    "        description=description,\n",
    "    ).launch(server_name=\"0.0.0.0\", server_port=7860)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\")\n",
    "app_path.write_text(app_code)\n",
    "print(f\"Wrote {app_path}\")\n",
    "\n",
    "src_checkpoint = Path(SAVE_PATH).expanduser()\n",
    "if src_checkpoint.exists():\n",
    "    dest_checkpoint = hf_dir / src_checkpoint.name\n",
    "    shutil.copy2(src_checkpoint, dest_checkpoint)\n",
    "    print(f\"Copied checkpoint to {dest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Warning: checkpoint not found locally; set WANDB_ARTIFACT so the Space can download weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e50df",
   "metadata": {},
   "source": [
    "### Create requirements.txt\n",
    "This requirements list is kept minimal for the Gradio app. Add pins if the Space needs reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b02a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote hf_space\\requirements.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requirements = \"\"\"\\\n",
    "torch\n",
    "torchvision\n",
    "gradio\n",
    "onnxruntime; sys_platform == 'win32'\n",
    "pillow\n",
    "wandb\n",
    "huggingface_hub\n",
    "git-lfs\n",
    "python-dotenv\n",
    "\"\"\"\n",
    "req_path = Path(\"hf_space\") / \"requirements.txt\"\n",
    "req_path.write_text(requirements.strip() + \"\\n\")\n",
    "print(f\"Wrote {req_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626a285",
   "metadata": {},
   "source": [
    "### Push to Hugging Face Space\n",
    "The next cell creates (or reuses) the target Space and uploads `app.py` and `requirements.txt`. It expects the environment variables you set above to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fab5719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space repo: https://huggingface.co/spaces/SaiRohith24816/tinyimagenet-demo\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7caf416f5d934076bb8a7640fbccb2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_resnet_wandb.pth:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded app.py and requirements.txt to the Space.\n",
      "Next: add WANDB_API_KEY as a secret on the Space if artifact download is required.\n",
      "\n",
      "Next: add WANDB_API_KEY as a secret on the Space if artifact download is required.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "hf_user = os.environ.get(\"HF_USER\")\n",
    "space_name = os.environ.get(\"SPACE_NAME\")\n",
    "if not hf_token or not hf_user or not space_name:\n",
    "    raise RuntimeError(\"HF_TOKEN, HF_USER, and SPACE_NAME must be set before pushing.\")\n",
    "\n",
    "repo_id = f\"{hf_user}/{space_name}\"\n",
    "api = HfApi(token=hf_token)\n",
    "repo_url = api.create_repo(repo_id=repo_id, repo_type=\"space\", space_sdk=\"gradio\", exist_ok=True)\n",
    "print(f\"Space repo: {repo_url}\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"hf_space\",\n",
    "    path_in_repo=\".\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"space\",\n",
    "    commit_message=\"Update Tiny-ImageNet Gradio app\"\n",
    " )\n",
    "print(\"Uploaded app.py and requirements.txt to the Space.\")\n",
    "print(\"Next: add WANDB_API_KEY as a secret on the Space if artifact download is required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5be9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple drift simulation functions loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image, ImageEnhance\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class BrightnessShift:\n",
    "    def __init__(self, factor=0.5):\n",
    "        self.factor = factor\n",
    "    def __call__(self, img):\n",
    "        return ImageEnhance.Brightness(img).enhance(self.factor)\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, std=0.1):\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std\n",
    "\n",
    "def get_drift_transform(img_size, drift_type, intensity):\n",
    "    transforms = [T.Resize((img_size, img_size))]\n",
    "    if drift_type == \"brightness\":\n",
    "        transforms.append(BrightnessShift(intensity))\n",
    "    transforms.append(T.ToTensor())\n",
    "    if drift_type == \"noise\":\n",
    "        transforms.append(AddGaussianNoise(intensity))\n",
    "    elif drift_type == \"combined\":\n",
    "        transforms.insert(-1, BrightnessShift(0.4))\n",
    "        transforms.append(AddGaussianNoise(0.15))\n",
    "    transforms.append(T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    class_names = ckpt.get(\"class_names\", [])\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(class_names))\n",
    "    model.load_state_dict(ckpt.get(\"model_state\", ckpt))\n",
    "    model.to(device).eval()\n",
    "    return model, class_names, device\n",
    "\n",
    "def create_dataset(data_dir, class_names, num_samples=200):\n",
    "    data_dir = Path(data_dir)\n",
    "    samples = []\n",
    "    with open(data_dir / \"val\" / \"val_annotations.txt\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                img_path = data_dir / \"val\" / \"images\" / parts[0]\n",
    "                if img_path.exists():\n",
    "                    samples.append((img_path, parts[1]))\n",
    "    random.seed(42)\n",
    "    samples = random.sample(samples, min(num_samples, len(samples)))\n",
    "    dataset = []\n",
    "    for img_path, wnid in samples:\n",
    "        try:\n",
    "            dataset.append((Image.open(img_path).convert(\"RGB\"), class_names.index(wnid)))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return dataset\n",
    "\n",
    "def evaluate(model, dataset, transform, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(dataset, leave=False):\n",
    "            x = transform(img).unsqueeze(0).to(device)\n",
    "            y = torch.tensor([label], dtype=torch.long).to(device)\n",
    "            out = model(x)\n",
    "            loss += criterion(out, y).item()\n",
    "            correct += (out.argmax(1).item() == label)\n",
    "            total += 1\n",
    "    return correct / total, loss / total\n",
    "\n",
    "print(\"Simple drift simulation functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7e3f1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024630-fzaqhacr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fzaqhacr' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fzaqhacr' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fzaqhacr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.18</td></tr><tr><td>baseline_accuracy</td><td>0.18</td></tr><tr><td>baseline_loss</td><td>4.16186</td></tr><tr><td>loss</td><td>4.16186</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fzaqhacr' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fzaqhacr</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024630-fzaqhacr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: Accuracy=0.1800, Loss=4.1619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, class_names, device = load_model(\"best_resnet_wandb.pth\")\n",
    "dataset = create_dataset(\"./tiny-imagenet-200\", class_names, num_samples=200)\n",
    "baseline_transform = T.Compose([\n",
    "    T.Resize((64, 64)), T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "wandb.init(project=\"tinyimagenet-resnet\", entity=\"ir2023\", name=\"baseline\", \n",
    "           tags=[\"drift\"], config={\"samples\": len(dataset)})\n",
    "baseline_acc, baseline_loss = evaluate(model, dataset, baseline_transform, device)\n",
    "wandb.log({\"accuracy\": baseline_acc, \"loss\": baseline_loss})\n",
    "wandb.summary.update({\"baseline_accuracy\": baseline_acc, \"baseline_loss\": baseline_loss})\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"Baseline: Accuracy={baseline_acc:.4f}, Loss={baseline_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "848d04f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024636-k3oy28qu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/k3oy28qu' target=\"_blank\">drift_dark_30</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/k3oy28qu' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/k3oy28qu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>accuracy_drop</td><td>▁</td></tr><tr><td>accuracy_drop_percent</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.105</td></tr><tr><td>accuracy_drop</td><td>0.075</td></tr><tr><td>accuracy_drop_percent</td><td>41.66667</td></tr><tr><td>alert_triggered</td><td>False</td></tr><tr><td>loss</td><td>4.61847</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drift_dark_30</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/k3oy28qu' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/k3oy28qu</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024636-k3oy28qu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dark_30: Acc=0.1050, Drop=0.0750 (✓)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024641-utaybjbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/utaybjbj' target=\"_blank\">drift_dark_20</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/utaybjbj' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/utaybjbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>accuracy_drop</td><td>▁</td></tr><tr><td>accuracy_drop_percent</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.065</td></tr><tr><td>accuracy_drop</td><td>0.115</td></tr><tr><td>accuracy_drop_percent</td><td>63.88889</td></tr><tr><td>alert_triggered</td><td>True</td></tr><tr><td>loss</td><td>4.84289</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drift_dark_20</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/utaybjbj' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/utaybjbj</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024641-utaybjbj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dark_20: Acc=0.0650, Drop=0.1150 (⚠️)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024647-fvpwuh26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fvpwuh26' target=\"_blank\">drift_bright_180</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fvpwuh26' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fvpwuh26</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>accuracy_drop</td><td>▁</td></tr><tr><td>accuracy_drop_percent</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.145</td></tr><tr><td>accuracy_drop</td><td>0.035</td></tr><tr><td>accuracy_drop_percent</td><td>19.44444</td></tr><tr><td>alert_triggered</td><td>False</td></tr><tr><td>loss</td><td>4.31817</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drift_bright_180</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fvpwuh26' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/fvpwuh26</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024647-fvpwuh26\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bright_180: Acc=0.1450, Drop=0.0350 (✓)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024652-1o3ykngs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/1o3ykngs' target=\"_blank\">drift_noise_low</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/1o3ykngs' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/1o3ykngs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>accuracy_drop</td><td>▁</td></tr><tr><td>accuracy_drop_percent</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.11</td></tr><tr><td>accuracy_drop</td><td>0.07</td></tr><tr><td>accuracy_drop_percent</td><td>38.88889</td></tr><tr><td>alert_triggered</td><td>False</td></tr><tr><td>loss</td><td>4.50471</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drift_noise_low</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/1o3ykngs' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/1o3ykngs</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024652-1o3ykngs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_low: Acc=0.1100, Drop=0.0700 (✓)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024657-uavvlian</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/uavvlian' target=\"_blank\">drift_noise_high</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/uavvlian' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/uavvlian</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>accuracy_drop</td><td>▁</td></tr><tr><td>accuracy_drop_percent</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.03</td></tr><tr><td>accuracy_drop</td><td>0.15</td></tr><tr><td>accuracy_drop_percent</td><td>83.33333</td></tr><tr><td>alert_triggered</td><td>True</td></tr><tr><td>loss</td><td>5.33197</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drift_noise_high</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/uavvlian' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/uavvlian</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024657-uavvlian\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_high: Acc=0.0300, Drop=0.1500 (⚠️)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sairo\\Desktop\\MLOps\\mlops2025w_142201019\\assignments\\assignment_6\\wandb\\run-20251111_024702-mc0rtr4g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/mc0rtr4g' target=\"_blank\">drift_combined</a></strong> to <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/mc0rtr4g' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/mc0rtr4g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>accuracy_drop</td><td>▁</td></tr><tr><td>accuracy_drop_percent</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.02</td></tr><tr><td>accuracy_drop</td><td>0.16</td></tr><tr><td>accuracy_drop_percent</td><td>88.88889</td></tr><tr><td>alert_triggered</td><td>True</td></tr><tr><td>loss</td><td>5.24387</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drift_combined</strong> at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet/runs/mc0rtr4g' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet/runs/mc0rtr4g</a><br> View project at: <a href='https://wandb.ai/ir2023/tinyimagenet-resnet' target=\"_blank\">https://wandb.ai/ir2023/tinyimagenet-resnet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_024702-mc0rtr4g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined: Acc=0.0200, Drop=0.1600 (⚠️)\n",
      "\n",
      "Completed 6 drift scenarios\n",
      "\n",
      "\n",
      "Completed 6 drift scenarios\n"
     ]
    }
   ],
   "source": [
    "scenarios = [\n",
    "    {\"name\": \"dark_30\", \"type\": \"brightness\", \"intensity\": 0.3},\n",
    "    {\"name\": \"dark_20\", \"type\": \"brightness\", \"intensity\": 0.2},\n",
    "    {\"name\": \"bright_180\", \"type\": \"brightness\", \"intensity\": 1.8},\n",
    "    {\"name\": \"noise_low\", \"type\": \"noise\", \"intensity\": 0.1},\n",
    "    {\"name\": \"noise_high\", \"type\": \"noise\", \"intensity\": 0.25},\n",
    "    {\"name\": \"combined\", \"type\": \"combined\", \"intensity\": 0.0},\n",
    "]\n",
    "\n",
    "results = []\n",
    "threshold = 0.1\n",
    "\n",
    "for s in scenarios:\n",
    "    transform = get_drift_transform(64, s['type'], s['intensity'])\n",
    "    run = wandb.init(project=\"tinyimagenet-resnet\", entity=\"ir2023\", \n",
    "                     name=f\"drift_{s['name']}\", tags=[\"drift\", s['type']], \n",
    "                     config={**s, \"baseline_acc\": baseline_acc})\n",
    "    \n",
    "    acc, loss = evaluate(model, dataset, transform, device)\n",
    "    drop = baseline_acc - acc\n",
    "    drop_pct = (drop / baseline_acc * 100) if baseline_acc > 0 else 0\n",
    "    alert = drop > threshold\n",
    "    \n",
    "    wandb.log({\"accuracy\": acc, \"loss\": loss, \"accuracy_drop\": drop, \n",
    "               \"accuracy_drop_percent\": drop_pct})\n",
    "    wandb.summary.update({\"accuracy\": acc, \"accuracy_drop\": drop, \n",
    "                          \"alert_triggered\": alert})\n",
    "    \n",
    "    if alert:\n",
    "        wandb.alert(title=f\"Drift: {s['name']}\", \n",
    "                    text=f\"Acc drop: {drop:.4f} ({drop_pct:.2f}%)\",\n",
    "                    level=wandb.AlertLevel.WARN)\n",
    "    \n",
    "    results.append({\"scenario\": s['name'], \"accuracy\": acc, \"loss\": loss,\n",
    "                    \"drop\": drop, \"drop_pct\": drop_pct, \"alert\": alert, \n",
    "                    \"url\": run.url})\n",
    "    wandb.finish()\n",
    "    print(f\"{s['name']}: Acc={acc:.4f}, Drop={drop:.4f} ({'⚠️' if alert else '✓'})\")\n",
    "\n",
    "print(f\"\\nCompleted {len(scenarios)} drift scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "188d3301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drift Results Summary:\n",
      "  scenario  accuracy  drop  drop_pct  alert\n",
      "   dark_30     0.105 0.075 41.666667  False\n",
      "   dark_20     0.065 0.115 63.888889   True\n",
      "bright_180     0.145 0.035 19.444444  False\n",
      " noise_low     0.110 0.070 38.888889  False\n",
      "noise_high     0.030 0.150 83.333333   True\n",
      "  combined     0.020 0.160 88.888889   True\n",
      "\n",
      "Worst: combined (drop: 0.1600)\n",
      "\n",
      "  scenario  accuracy  drop  drop_pct  alert\n",
      "   dark_30     0.105 0.075 41.666667  False\n",
      "   dark_20     0.065 0.115 63.888889   True\n",
      "bright_180     0.145 0.035 19.444444  False\n",
      " noise_low     0.110 0.070 38.888889  False\n",
      "noise_high     0.030 0.150 83.333333   True\n",
      "  combined     0.020 0.160 88.888889   True\n",
      "\n",
      "Worst: combined (drop: 0.1600)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print(\"\\nDrift Results Summary:\")\n",
    "print(df[['scenario', 'accuracy', 'drop', 'drop_pct', 'alert']].to_string(index=False))\n",
    "print(f\"\\nWorst: {df.loc[df['drop'].idxmax(), 'scenario']} (drop: {df['drop'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf7d457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: drift_simple_report.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = f\"\"\"# Drift Report\n",
    "\n",
    "**Baseline:** {baseline_acc:.4f} | **Threshold:** {threshold}\n",
    "**Samples:** {len(dataset)} | **Alerts:** {df['alert'].sum()}\n",
    "\n",
    "## Results\n",
    "{df[['scenario', 'accuracy', 'drop', 'drop_pct', 'alert']].to_markdown(index=False)}\n",
    "\n",
    "## W&B Links\n",
    "\"\"\"\n",
    "for _, r in df.iterrows():\n",
    "    report += f\"- [{r['scenario']}]({r['url']})\\n\"\n",
    "\n",
    "Path(\"drift_report.md\").write_text(report, encoding='utf-8')\n",
    "print(\"Saved: drift_simple_report.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
